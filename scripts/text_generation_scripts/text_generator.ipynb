{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding=utf-8\n",
    "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#Conditional text generation with the auto-regressive models of the library (GPT/GPT-2/CTRL/Transformer-XL/XLNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_length_to_model(length, max_sequence_length):\n",
    "    if length < 0 and max_sequence_length > 0:\n",
    "        length = max_sequence_length\n",
    "    elif 0 < max_sequence_length < length:\n",
    "        length = max_sequence_length  # No generation bigger than model size\n",
    "    elif length < 0:\n",
    "        length = MAX_LENGTH  # avoid infinite loop\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**kwarg):\n",
    "    \n",
    "    no_cuda = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    try:\n",
    "        model_class, tokenizer_class = MODEL_CLASSES[kwarg['model_type']]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(kwarg['model_name_or_path'])\n",
    "    model = model_class.from_pretrained(kwarg['model_name_or_path'])\n",
    "\n",
    "    length = adjust_length_to_model(kwarg['length'], max_sequence_length=model.config.max_position_embeddings)\n",
    "    logger.info(kwarg)\n",
    "\n",
    "    prompt_text = kwarg['prompt'] if kwarg['prompt'] else input(\"Model prompt >>> \")\n",
    "    \n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "    if encoded_prompt.size()[-1] == 0:\n",
    "        input_ids = None\n",
    "    else:\n",
    "        input_ids = encoded_prompt\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=length + len(encoded_prompt[0]),\n",
    "        temperature= 1.0,\n",
    "        top_k = kwarg['k'],\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        do_sample=True,\n",
    "        num_return_sequences = 2\n",
    "    )\n",
    "\n",
    "    # Remove the batch dimension when returning multiple sequences\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(kwarg['stop_token']) if kwarg['stop_token'] else None]\n",
    "\n",
    "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "        total_sequence = (\n",
    "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "        )\n",
    "\n",
    "        generated_sequences.append(total_sequence)\n",
    "        print(total_sequence)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR='./output'\n",
    "TRAIN_FILE='./dataset/prepared-data/text-generation/train_text-generation.txt'\n",
    "VALID_FILE='./dataset/prepared-data/text-generation/dev_text-generation.txt'\n",
    "PROMPT = \"certain genetic conditions increase the risk of childhood cns embryonal tumors .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/26/2020 15:04:27 - INFO - __main__ -   {'model_type': 'gpt2', 'model_name_or_path': './output', 'length': 300, 'prompt': 'certain genetic conditions increase the risk of childhood cns embryonal tumors .', 'stop_token': '<EOS>', 'k': 30, 'num_return_sequences': 2}\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SEQUENCE 1 ===\n",
      "certain genetic conditions increase the risk of childhood cns embryonal tumors . A childhood carcinoma may cause signs or symptoms that begin before the tumor is diagnosed and continue for months or years.   Childhood cancer is usually treated as childhood cns embryonal tumors:           - Tissue is removed and the tumor is removed by surgery or chemotherapy.    - The tumor is raised under an artificial light (a fluorescent light that emits light) so that it is more visible to the outside world.    - The tumor is placed in a sealed bag or plastic bag with wires to carry radioactive material <BOS>  during surgery or chemotherapy.     - A special pathologist views the tumor, checks it, and removes any tumor cells that are not cancer.       Tests that examine the body and bones are used to detect (find) and diagnose childhood carcinomas.      Check the list of NCI-supported cancer clinical trials that are now accepting patients with childhood cns embryonal tumors. For more specific results, refine the search by using other search features, such as the location of the trial, the type of treatment, or the name of the drug. Talk with your child's doctor about clinical trials that may be right for your child. General information about clinical trials is available from the NCI website. \n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "certain genetic conditions increase the risk of childhood cns embryonal tumors . Genetic risk factors for neuroblastoma include smoking, drinking alcohol, and not being able to walk. Other factors increase the risk of neuroblastoma.   Childhood brain tumors include the following:          -  Brain tumors (the most common type of brain tumor).     -  Non-small cell lung cancer.      -  Hodgkin lymphoma.       Childhood neuroblastoma may have a protective effect on the tumor's primary tumor:          - The tumor is more likely to be found in the head or neck than in the pelvis.      - The tumor is more likely to spread to the cerebrospinal fluid (CSF), the lymph vessels, and the brain.     - The tumor may be bigger or more than 3 centimeters (3 inches) tall.     - The tumor may be diagnosed as childhood neuroblastoma or as recurrent childhood neuroblastoma. \n",
      "CPU times: user 1min 23s, sys: 2.44 s, total: 1min 25s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = main(model_type ='gpt2',\n",
    "              model_name_or_path = OUTPUT_DIR,\n",
    "              length = 300,\n",
    "              prompt = PROMPT,\n",
    "              stop_token = \"<EOS>\",\n",
    "              k = 30,\n",
    "              num_return_sequences = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune GPT2 language model to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-23 16:41:02.134525: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/cuda10.0/toolkit/10.0.130/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda10.0/toolkit/10.0.130/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2020-11-23 16:41:02.134594: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "11/23/2020 16:41:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/23/2020 16:41:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='./output/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov23_16-41-23_c0097', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='./output/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
      "/home/mtsami/.local/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "/home/mtsami/.local/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:114: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "{'loss': 2.53866455078125, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.2777777777777778}\n",
      "{'loss': 1.32922119140625, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.5555555555555556}\n",
      "{'loss': 1.172187255859375, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.8333333333333334}\n",
      "{'epoch': 1.0}                                                                  \n",
      "100%|███████████████████████████████████████| 1800/1800 [03:36<00:00,  8.33it/s]\n",
      "11/23/2020 16:45:10 - INFO - __main__ -   *** Evaluate ***\n",
      "100%|█████████████████████████████████████████| 270/270 [00:07<00:00, 35.00it/s]\n",
      "11/23/2020 16:45:17 - INFO - __main__ -   ***** Eval results *****\n",
      "11/23/2020 16:45:17 - INFO - __main__ -     perplexity = 2.716787746231871\n",
      "CPU times: user 2.49 s, sys: 545 ms, total: 3.03 s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python3 ./transformers/examples/language-modeling/run_language_modeling.py \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--model_type gpt2\\\n",
    "--model_name_or_path gpt2\\\n",
    "--do_train \\\n",
    "--train_data_file $TRAIN_FILE \\\n",
    "--do_eval \\\n",
    "--eval_data_file $VALID_FILE \\\n",
    "--per_device_train_batch_size=2 \\\n",
    "--per_device_eval_batch_size=2 \\\n",
    "--line_by_line \\\n",
    "--overwrite_output_dir\\\n",
    "--learning_rate=5e-5 \\\n",
    "--num_train_epochs=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepNLP2020]",
   "language": "python",
   "name": "conda-env-deepNLP2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
