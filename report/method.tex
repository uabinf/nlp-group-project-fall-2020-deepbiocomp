\section*{Method}
	The following sections are categorized into three main sections, first a brief explanation on the data processing and implementation methods used to fine-tune the BioBERT model for regular question answering. Second, the data processing and fine-tuning of the GPT2 model for text-generation which is the basis for creation of long comprehensive answers. Finally, the report discusses the implementation methods for a composite model that runs the fine-tuned BioBERT for Question-Answering and uses the output of the BioBERT model as the input prompt for the GPT2 text generator for a more verbose, comprehensive answer. 
	
	\subsection*{Fine-tuning BioBERT}
		The biomedical domain texts contain a vast number of domain-specific proper nouns (e.g.  BRCA1, Leukemia) which are understood mostly by biomedical researchers. In this context the   BioBERT is already fine-tuned on PubMed abstracts (PubMed) and PubMed Central full-text articles(PMC). Further, we fine-tuned again using Cancer QA dataset, the MedQuAD. The total Cancer and question type is summarized in Figure~\ref{fig:cancer-type-qtype-count}. 
		
	\subsection*{Fine-tuning GPT2}
		This section will briefly explain the data used to pretraint the GPT2 model and how the script that was implemented to use the GPT2 model for text generation. The GPT2 model was chosen for this task because of the robustness of the model and its ability to generate long sentences while maintaining relatively good semantic sense. However, the language model is much too general and requires fine-tuning to work effectively and generative texts pertinent to cancer queries. Thus, the model was fine-tuned on the same MedQuAD dataset that was used to fine-tune the aforementioned BioBERT model.
		
		However, unlike the BioBERT model which is used for question-answering, the data must be processed differently for the GPT2 model. In the case of the question-answering models, the dataset usually has three major components, the `question', `context' and the `answer'. All of which are important for training a question-answering model, but fine-tuning a language model (GPT2) does not require all three components. Concretely, the GPT2 model was trained only using the `context' of the dataset. Furthermore,  two special tokens were added, the `<BOS>' signifying the beginning of a sentence and a `<EOS>' token, signifying the end of a sentence. The code for fine-tuning the GPT2 model is based on an older (not currently available) script from the huggingface transformer git repo called `run\_language\_modelling.py'. The script was modified and reimplemented as a jupyter notebook. Furthermore, it was also modified to let GPT2 accept the special tokens as mentioned above. Once the model was fine-tuned with our desired dataset, it was saved locally to be used for can related text-generation.
		
	\subsection*{Composite Model for Comprehensive Question Answering}
		The final section will discuss the composite model that stacks the fine-tuned GPT2 based text-generation model on top of  the fine-tuned BioBERT model for question-answering. The question-answering code is based on the `run\_squad.py' script from the Huggingface’s Transformer git repository. The fine-tuned model takes as input the question provided by the user and tries to give an answer that is correct and contextually relevant to the question asked. Once, the BioBERT model returns an answer for the given query, the output is used as the input for the GPT2 model. The text-generation code is based on the `run\_generation.py' script from the Huggingface’s Transformer git repository. The script has been heavily modified and rewritten as a python function in a jupyter notebook. It has also been modified to accept our dataset which contains the `<BOS>' and `<EOS>' special tokens. The text-generation model returns two suitable answers which are generated based on the prompt provided from the question-answering model. The resulting final answer is not only verbose and comprehensive but semantically and contextually relevant to the question asked. Thus, providing a much better experience to the user submitting the queries to the model.