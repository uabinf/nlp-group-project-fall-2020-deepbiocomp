{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import list_datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "#datasets_list = list_datasets()\n",
    "#len(datasets_list)\n",
    "#print(', '.join(dataset.id for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+git://github.com/huggingface/transformers/\n",
      "  Cloning git://github.com/huggingface/transformers/ to /local/pip-req-build-05xkjnk9\n",
      "  Running command git clone -q git://github.com/huggingface/transformers/ /local/pip-req-build-05xkjnk9\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): transformers==4.0.0rc1 from git+git://github.com/huggingface/transformers/ in /home/nileshkr/.local/lib/python3.8/site-packages\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/nileshkr/.local/lib/python3.8/site-packages (from transformers==4.0.0rc1) (0.9.4)\n",
      "Requirement already satisfied: numpy in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers==4.0.0rc1) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers==4.0.0rc1) (2020.6.8)\n",
      "Requirement already satisfied: sacremoses in /home/nileshkr/.local/lib/python3.8/site-packages (from transformers==4.0.0rc1) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers==4.0.0rc1) (4.47.0)\n",
      "Requirement already satisfied: requests in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers==4.0.0rc1) (2.24.0)\n",
      "Requirement already satisfied: filelock in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers==4.0.0rc1) (3.0.12)\n",
      "Requirement already satisfied: packaging in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers==4.0.0rc1) (20.4)\n",
      "Requirement already satisfied: click in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from sacremoses->transformers==4.0.0rc1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from sacremoses->transformers==4.0.0rc1) (0.16.0)\n",
      "Requirement already satisfied: six in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from sacremoses->transformers==4.0.0rc1) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers==4.0.0rc1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers==4.0.0rc1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers==4.0.0rc1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers==4.0.0rc1) (1.25.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from packaging->transformers==4.0.0rc1) (2.4.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.0.0rc1-py3-none-any.whl size=1357994 sha256=79edd085c1a24455d7c13e483bafb784413b4fa952e77f68d54046aaaa893acc\n",
      "  Stored in directory: /local/pip-ephem-wheel-cache-0bxbltk8/wheels/cd/ea/91/809bbe697d6027d023e294d575386b91adf4a1e28f1fbd3f89\n",
      "Successfully built transformers\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/nlp.git\n",
      "  Cloning https://github.com/huggingface/nlp.git to /local/pip-req-build-einh6oua\n",
      "  Running command git clone -q https://github.com/huggingface/nlp.git /local/pip-req-build-einh6oua\n",
      "Requirement already satisfied (use --upgrade to upgrade): datasets==1.1.3 from git+https://github.com/huggingface/nlp.git in /home/nileshkr/.local/lib/python3.8/site-packages\n",
      "Requirement already satisfied: numpy>=1.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.1.3) (1.18.5)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/nileshkr/.local/lib/python3.8/site-packages (from datasets==1.1.3) (2.0.0)\n",
      "Requirement already satisfied: dill in /home/nileshkr/.local/lib/python3.8/site-packages (from datasets==1.1.3) (0.3.3)\n",
      "Requirement already satisfied: pandas in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.1.3) (1.0.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.1.3) (2.24.0)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.1.3) (4.47.0)\n",
      "Requirement already satisfied: xxhash in /home/nileshkr/.local/lib/python3.8/site-packages (from datasets==1.1.3) (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/nileshkr/.local/lib/python3.8/site-packages (from datasets==1.1.3) (0.70.11.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from pandas->datasets==1.1.3) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from pandas->datasets==1.1.3) (2.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.1.3) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.1.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.1.3) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->datasets==1.1.3) (1.15.0)\n",
      "Building wheels for collected packages: datasets\n",
      "  Building wheel for datasets (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for datasets: filename=datasets-1.1.3-py3-none-any.whl size=157232 sha256=b28fa5ea06c92dc15b13767678083f269a61a7aacd2f875259c8eb5382b7add0\n",
      "  Stored in directory: /local/pip-ephem-wheel-cache-tf28fk86/wheels/38/7e/f4/c9c3f0bb25fd1ac84f68a9489eb1bf9db2233f2c48ef8312b8\n",
      "Successfully built datasets\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/huggingface/transformers.git\n",
    "!pip install git+git://github.com/huggingface/transformers/\n",
    "#!pip install -U ./transformers\n",
    "!pip install git+https://github.com/huggingface/nlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install nlp\n",
    "import torch\n",
    "import nlp\n",
    "from transformers import LongformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall transformers==3.3.1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
    "    gold_text = answer['text'][0]\n",
    "    start_idx = answer['answer_start'][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "        return start_idx, end_idx       # When the gold label position is good\n",
    "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "        return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "        return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    else:\n",
    "        if not start_idx:\n",
    "            #print(start_idx, end_idx)\n",
    "            start_idx = 1\n",
    "        if not end_idx:\n",
    "            end_idx = 1\n",
    "        return start_idx, end_idx\n",
    "        raise ValueError()\n",
    "        \n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    input_pairs = [example['question'], example['context']]\n",
    "    encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, max_length=512)\n",
    "    context_encodings = tokenizer.encode_plus(example['context'])\n",
    "    \n",
    "\n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "    # this will give us the position of answer span in the context text\n",
    "    start_idx, end_idx = get_correct_alignement(example['context'], example['answers'])\n",
    "    start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "    end_positions_context = context_encodings.char_to_token(end_idx-1)\n",
    "\n",
    "    # here we will compute the start and end position of the answer in the whole example\n",
    "    # as the example is encoded like this <s> question</s></s> context</s>\n",
    "    # and we know the postion of the answer in the context\n",
    "    # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "    # this will give us the position of the answer span in whole example \n",
    "    sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "    start_positions = start_positions_context + sep_idx + 1\n",
    "    end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "    if end_positions > 512:\n",
    "      start_positions, end_positions = 0, 0\n",
    "\n",
    "    encodings.update({'start_positions': start_positions,\n",
    "                      'end_positions': end_positions,\n",
    "                      'attention_mask': encodings['attention_mask']})\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/nileshkr/.cache/huggingface/datasets/squad/plain_text/1.0.0/940a7b3b750e3156d9774d66c3d26678f4fd3269f3807c52ba03a854c9d43ac5)\n",
      "Reusing dataset squad (/home/nileshkr/.cache/huggingface/datasets/squad/plain_text/1.0.0/940a7b3b750e3156d9774d66c3d26678f4fd3269f3807c52ba03a854c9d43ac5)\n",
      "Loading cached processed dataset at /home/nileshkr/.cache/huggingface/datasets/squad/plain_text/1.0.0/940a7b3b750e3156d9774d66c3d26678f4fd3269f3807c52ba03a854c9d43ac5/cache-08f2a9bcf54c4fc4.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde266bb7ea8482b820d4486a9d187ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#dataset = load_dataset('cancer/squad.py', split=nlp.Split.TRAIN)\n",
    "\n",
    "\n",
    "# load train and validation split of squad\n",
    "train_dataset  = load_dataset('cancer/squad.py', split=nlp.Split.TRAIN)\n",
    "valid_dataset = load_dataset('cancer/squad.py',  split=nlp.Split.VALIDATION)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(convert_to_features)\n",
    "valid_dataset = valid_dataset.map(convert_to_features, load_from_cache_file=False)\n",
    "\n",
    "\n",
    "# set the tensor type and the columns which the dataset should return\n",
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(583, 73)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cach the dataset, so we can load it directly for training\n",
    "\n",
    "torch.save(train_dataset, 'train_data.pt')\n",
    "torch.save(valid_dataset, 'valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import LongformerForQuestionAnswering, LongformerTokenizerFast, EvalPrediction\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DummyDataCollator:#(DataCollator):\n",
    "    def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        start_positions = torch.stack([example['start_positions'] for example in batch])\n",
    "        end_positions = torch.stack([example['end_positions'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'start_positions': start_positions, \n",
    "            'end_positions': end_positions,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    train_file_path: Optional[str] = field(\n",
    "        default='train_data.pt',\n",
    "        metadata={\"help\": \"Path for cached train dataset\"},\n",
    "    )\n",
    "    valid_file_path: Optional[str] = field(\n",
    "        default='valid_data.pt',\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"},\n",
    "    )\n",
    "    max_len: Optional[int] = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Max input length for the source text\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "\n",
    "    # we will load the arguments from a json file, \n",
    "    # make sure you save the arguments in at ./args.json\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    tokenizer = LongformerTokenizerFast.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    model = LongformerForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    # Get datasets\n",
    "    print('loading data')\n",
    "    train_dataset  = torch.load(data_args.train_file_path)\n",
    "    valid_dataset = torch.load(data_args.valid_file_path)\n",
    "    print('loading done')\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=DummyDataCollator(),\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval and training_args.local_rank in [-1, 0]:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(eval_output.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "    \n",
    "        results.update(eval_output)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "  \"n_gpu\": 1,\n",
    "  \"model_name_or_path\": 'allenai/longformer-base-4096',\n",
    "  \"max_len\": 512 ,\n",
    "  \"output_dir\": './models',\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"per_gpu_train_batch_size\": 8,\n",
    "  \"per_gpu_eval_batch_size\": 8,\n",
    "  \"gradient_accumulation_steps\": 16,\n",
    "  \"learning_rate\": 1e-4,\n",
    "  \"num_train_epochs\": 3,\n",
    "  \"do_train\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('args.json', 'w') as f:\n",
    "  json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2020 02:29:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/27/2020 02:29:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='./models', overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov27_02-29-16_c0107', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='./models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "loading done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'prediction_loss_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-79ad255ba134>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Initialize our Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     trainer = Trainer(\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'prediction_loss_only'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQuAD evaluation script. Modifed slightly for this notebook\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "      total += 1\n",
    "      exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "      f1 += metric_max_over_ground_truths(\n",
    "          f1_score, prediction, ground_truths)\n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizerFast, LongformerForQuestionAnswering\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'models'. Make sure that:\n\n- 'models' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'models' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-528a1eba703d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLongformerTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLongformerForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m             )\n\u001b[0;32m-> 1776\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'models'. Make sure that:\n\n- 'models' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'models' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('models')\n",
    "model = LongformerForQuestionAnswering.from_pretrained('models')\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = torch.load('valid_data.pt')\n",
    "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(dataloader):\n",
    "    start_scores, end_scores = model(input_ids=batch['input_ids'].cuda(),\n",
    "                                  attention_mask=batch['attention_mask'].cuda())\n",
    "    for i in range(start_scores.shape[0]):\n",
    "      all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n",
    "      answer = ' '.join(all_tokens[torch.argmax(start_scores[i]) : torch.argmax(end_scores[i])+1])\n",
    "      ans_ids = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "      answer = tokenizer.decode(ans_ids)\n",
    "      answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "for ref, pred in zip(valid_dataset, answers):\n",
    "  predictions.append(pred)\n",
    "  references.append(ref['answers']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Surgery Radiation therapy Chemotherapy Biologic therapy'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "model = LongformerForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "\n",
    "text = \"Key Points There are different types of treatment for patients with vulvar cancer. Four types of standard treatment are used: Surgery Radiation therapy Chemotherapy Biologic therapy New types of treatment are being tested in clinical trials. Patients may want to think about taking part in a clinical trial. Patients can enter clinical trials before, during, or after starting their cancer treatment. Followup tests may be needed. There are different types of treatment for patients with vulvar cancer. Different types of treatments are available for patients with vulvar cancer. Some treatments are standard (the currently used treatment), and some are being tested in clinical trials. A treatment clinical trial is a research study meant to help improve current treatments or obtain information on new treatments for patients with cancer. When clinical trials show that a new treatment is better than the standard treatment, the new treatment may become the standard treatment.\"\n",
    "question = \"What are the treatments for Vulvar Cancer?\"\n",
    "encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "# default is local attention everywhere\n",
    "# the forward method will automatically set global attention on question tokens\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "start_scores = model(input_ids, attention_mask=attention_mask)[0]\n",
    "end_scores = model(input_ids, attention_mask=attention_mask)[1]\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n",
    "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "answer\n",
    "# output => democratized NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
